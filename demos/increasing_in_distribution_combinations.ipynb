{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e64acb",
   "metadata": {
    "id": "r0Gjg0FG3Lh8"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Spandan-Madan/generalization_biased_category_pose/blob/main/demos/increasing_seen_combinations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e5c54",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "This demo shows the impact of increasing in-distribution combinations on out-of-distribution generalization. Specifically, we reproduce the results for the MNIST Rotation dataset on the `SHARED` architecture.\n",
    "\n",
    "As shown below, this architecture enforces parameter sharing between the two tasks (category prediction and viewpoint prediction).\n",
    "\n",
    "As described in the paper, our results show that increasing data diversity (i.e. in-distribution combinations) leads to a substantial increase in out-of-distribution performance eventhough total number of training images (dataset size) is held constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca791caa",
   "metadata": {},
   "source": [
    "![Shared Architecture](../docs/images/Shared.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-danish",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQAmV64ViCpP",
    "outputId": "17696cd0-553c-46bd-a5e2-76cbe592b9c7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def create_folder(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766dc560",
   "metadata": {},
   "source": [
    "If running on google colab, the below code does the following:\n",
    "- clone repo\n",
    "- set up necessary folders\n",
    "- download MNIST Rotation Dataset at appropriate place\n",
    "- unzip MNIST Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0fd8c",
   "metadata": {},
   "source": [
    "#### If you're not running on colab, please follow download instructions to get the mnist_rotaiton dataset using:\n",
    "\n",
    "```\n",
    "cd utils\n",
    "bash download_mnist_rotation.sh\n",
    "```\n",
    "\n",
    "#### If not using google colab, please proceed below only after downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-singles",
   "metadata": {
    "id": "3TDIbS5ziEyi"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Cloning code base to colab....')\n",
    "    !git clone https://github.com/Spandan-Madan/generalization_to_OOD_category_viewpoint_cominations.git\n",
    "    !cd generalization_to_OOD_category_viewpoint_cominations/utils\n",
    "    !bash download_mnist_rotation.sh\n",
    "else:\n",
    "    CODE_ROOT = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-assessment",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hairy-growing",
    "outputId": "1bbc681b-bff8-4012-9e02-1be8c768c604"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "import random\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import argparse\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('%s/res/'%CODE_ROOT)\n",
    "from models.models import get_model\n",
    "from loader.loader import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-device",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "european-influence",
    "outputId": "8fc8e967-2579-44f7-8e49-a5e5d48f9e84"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_palette(\"Set1\", 8, .75)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4439267",
   "metadata": {},
   "source": [
    "This demo trains networks with 1, 3, 6 and 9 in-distribution combinations of the MNIST-Rotation dataset, and plots performance on out-of-distribution combinations from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28193b",
   "metadata": {},
   "source": [
    "To run on a different architecture, please change the `ARCH` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-warrior",
   "metadata": {
    "id": "suspected-fever"
   },
   "outputs": [],
   "source": [
    "DATASET_NAMES = ['mnist_rotation_one_by_nine', 'mnist_rotation_three_by_nine',\n",
    "                 'mnist_rotation_six_by_nine', 'mnist_rotation_nine_by_nine']\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 200\n",
    "ARCH = 'LATE_BRANCHING_COMBINED'\n",
    "\n",
    "image_transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "\n",
    "GPU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-prague",
   "metadata": {
    "id": "judicial-simpson",
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = (10,10,10,10)\n",
    "loader_new = get_loader('multi_attribute_loader_file_list_mnist_rotation')\n",
    "\n",
    "file_list_root = '%s/dataset_lists/mnist_rotation_lists/'%CODE_ROOT\n",
    "att_path = '%s/dataset_lists/combined_attributes.p'%CODE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-diameter",
   "metadata": {
    "id": "blind-classroom"
   },
   "outputs": [],
   "source": [
    "shuffles = {'train':True,'val':True,'test':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-joseph",
   "metadata": {
    "id": "UGTQbDPvmn_G"
   },
   "outputs": [],
   "source": [
    "data_dir = '%s/data/'%CODE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-builder",
   "metadata": {
    "id": "through-competition"
   },
   "outputs": [],
   "source": [
    "all_dsets = {}\n",
    "all_dset_loaders = {}\n",
    "all_dset_sizes = {}\n",
    "\n",
    "for DATASET_NAME in DATASET_NAMES:\n",
    "    file_lists = {}\n",
    "    dsets = {}\n",
    "    dset_loaders = {}\n",
    "    dset_sizes = {}\n",
    "    for phase in ['train','val','test']:\n",
    "        file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,DATASET_NAME)\n",
    "        dsets[phase] = loader_new(file_lists[phase],att_path, image_transform, data_dir)\n",
    "        dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=BATCH_SIZE, shuffle = shuffles[phase], num_workers=2,drop_last=True)\n",
    "        dset_sizes[phase] = len(dsets[phase])\n",
    "    all_dsets[DATASET_NAME] = dsets\n",
    "    all_dset_loaders[DATASET_NAME] = dset_loaders\n",
    "    all_dset_sizes[DATASET_NAME] = dset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-edinburgh",
   "metadata": {
    "id": "wrong-allowance"
   },
   "outputs": [],
   "source": [
    "multi_losses = [nn.CrossEntropyLoss(),nn.CrossEntropyLoss(),nn.CrossEntropyLoss(),nn.CrossEntropyLoss()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-interface",
   "metadata": {
    "id": "killing-cause"
   },
   "outputs": [],
   "source": [
    "def weight_scheduler(epoch_num, task):\n",
    "    if task == 'shared':\n",
    "        return [0.0,1.0,0.0,1.0]\n",
    "    elif task == 'viewpoint':\n",
    "        return [0.0,1.0,0.0,0.0]\n",
    "    elif task == 'category':\n",
    "        return [0.0,0.0,0.0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-reminder",
   "metadata": {
    "id": "civil-bleeding"
   },
   "outputs": [],
   "source": [
    "def train_epoch(dset_loaders, dset_sizes, model, task, optimizer):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    phase = 'train'\n",
    "    \n",
    "    weights = weight_scheduler(epoch, task)\n",
    "    iters = 0\n",
    "    phase_epoch_corrects = [0,0,0,0]\n",
    "    phase_epoch_loss = 0\n",
    "    \n",
    "    for data in dset_loaders[phase]:\n",
    "        inputs, labels_all, paths = data\n",
    "        inputs = Variable(inputs.float().cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model_outs = model(inputs)\n",
    "        calculated_loss = 0\n",
    "        batch_corrects = [0,0,0,0]\n",
    "        \n",
    "        for i in range(4):\n",
    "            labels = labels_all[:,i]\n",
    "            if GPU:\n",
    "                labels = Variable(labels.long().cuda())\n",
    "            loss = multi_losses[i]\n",
    "            outputs = model_outs[i]\n",
    "            calculated_loss += weights[i] * loss(outputs,labels)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
    "            phase_epoch_corrects[i] += batch_corrects[i]\n",
    "\n",
    "        \n",
    "        phase_epoch_loss += calculated_loss\n",
    "        calculated_loss.backward()\n",
    "        optimizer.step()\n",
    "        iters += 1\n",
    "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
    "    # print('Train loss:%s'%epoch_loss)\n",
    "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
    "\n",
    "    if task == 'shared':\n",
    "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
    "    elif task == 'viewpoint':\n",
    "        epoch_gm = epoch_accs[1]\n",
    "    elif task == 'category':\n",
    "        epoch_gm = epoch_accs[3]\n",
    "    \n",
    "    return model, epoch_loss, epoch_gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-wisconsin",
   "metadata": {
    "id": "protecting-majority"
   },
   "outputs": [],
   "source": [
    "def test_epoch(dset_loaders, dset_sizes, model, best_model, best_test_loss, best_test_gm, task):\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    phase = 'val'\n",
    "    weights = weight_scheduler(epoch, task)\n",
    "    iters = 0\n",
    "    phase_epoch_corrects = [0,0,0,0]\n",
    "    phase_epoch_loss = 0\n",
    "    \n",
    "    for data in dset_loaders[phase]:\n",
    "        inputs, labels_all, paths = data\n",
    "        inputs = Variable(inputs.float().cuda())\n",
    "        model_outs = model(inputs)\n",
    "        calculated_loss = 0\n",
    "        batch_corrects = [0,0,0,0]\n",
    "        \n",
    "        for i in range(4):\n",
    "            labels = labels_all[:,i]\n",
    "            if GPU:\n",
    "                labels = Variable(labels.long().cuda())\n",
    "            loss = multi_losses[i]\n",
    "            outputs = model_outs[i]\n",
    "            calculated_loss += weights[i] * loss(outputs,labels)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
    "            phase_epoch_corrects[i] += batch_corrects[i]\n",
    "\n",
    "\n",
    "        phase_epoch_loss += calculated_loss\n",
    "        iters += 1\n",
    "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
    "    # print('Test loss:%s'%epoch_loss)\n",
    "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
    "    \n",
    "    if task == 'shared':\n",
    "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
    "    elif task == 'viewpoint':\n",
    "        epoch_gm = epoch_accs[1]\n",
    "    elif task == 'category':\n",
    "        epoch_gm = epoch_accs[3]\n",
    "    \n",
    "    if epoch_loss < best_test_loss:\n",
    "        best_model = model\n",
    "        best_test_loss = epoch_loss\n",
    "        best_test_gm = epoch_gm\n",
    "    \n",
    "    return best_model, epoch_loss, epoch_gm, best_test_loss, best_test_gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen_test_epoch(dset_loaders, dset_sizes, model, task):\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    phase = 'test'\n",
    "\n",
    "    weights = weight_scheduler(epoch, task)\n",
    "    iters = 0\n",
    "    phase_epoch_corrects = [0,0,0,0]\n",
    "    phase_epoch_loss = 0\n",
    "    \n",
    "    for data in dset_loaders[phase]:\n",
    "        inputs, labels_all, paths = data\n",
    "        inputs = Variable(inputs.float().cuda())\n",
    "        model_outs = model(inputs)\n",
    "        calculated_loss = 0\n",
    "        batch_corrects = [0,0,0,0]\n",
    "        \n",
    "        for i in range(4):\n",
    "            labels = labels_all[:,i]\n",
    "            if GPU:\n",
    "                labels = Variable(labels.long().cuda())\n",
    "            loss = multi_losses[i]\n",
    "            outputs = model_outs[i]\n",
    "            calculated_loss += weights[i] * loss(outputs,labels)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
    "            phase_epoch_corrects[i] += batch_corrects[i]\n",
    "\n",
    "\n",
    "        phase_epoch_loss += calculated_loss\n",
    "        iters += 1\n",
    "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
    "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
    "    \n",
    "    if task == 'shared':\n",
    "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
    "    elif task == 'viewpoint':\n",
    "        epoch_gm = epoch_accs[1]\n",
    "    elif task == 'category':\n",
    "        epoch_gm = epoch_accs[3]\n",
    "    \n",
    "    return epoch_loss, epoch_gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick', labelsize=14) \n",
    "plt.rc('ytick', labelsize=14) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-progress",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_titles = {}\n",
    "dataset_titles['mnist_rotation_one_by_nine'] = \"10% combinations seen\"\n",
    "dataset_titles['mnist_rotation_three_by_nine'] = \"30% combinations seen\"\n",
    "dataset_titles['mnist_rotation_six_by_nine'] = \"60% combinations seen\"\n",
    "dataset_titles['mnist_rotation_nine_by_nine'] = \"90% combinations seen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-listening",
   "metadata": {
    "id": "Fi_v4pE22eDi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for DATASET_NAME in DATASET_NAMES:\n",
    "    dsets = all_dsets[DATASET_NAME]\n",
    "    dset_loaders = all_dset_loaders[DATASET_NAME]\n",
    "    dset_sizes = all_dset_sizes[DATASET_NAME]\n",
    "    \n",
    "    models = {}\n",
    "\n",
    "    models['shared']= get_model(ARCH,NUM_CLASSES)\n",
    "    models['viewpoint']= get_model(ARCH,NUM_CLASSES)\n",
    "    models['category']= get_model(ARCH,NUM_CLASSES)\n",
    "\n",
    "    models['shared'].cuda();\n",
    "    models['viewpoint'].cuda();\n",
    "    models['category'].cuda();\n",
    "\n",
    "    best_models = {}\n",
    "    best_models['shared'] = models['shared']\n",
    "    best_models['viewpoint'] = models['viewpoint']\n",
    "    best_models['category'] = models['category']\n",
    "\n",
    "    best_test_loss = 100\n",
    "    best_test_gm = 0\n",
    "\n",
    "    all_train_gms = {}\n",
    "    all_train_gms['shared'] = [0]\n",
    "    all_train_gms['separate'] = [0]\n",
    "\n",
    "    all_test_gms = {}\n",
    "    all_test_gms['shared'] = [0]\n",
    "    all_test_gms['separate'] = [0]\n",
    "\n",
    "    all_unseen_test_gms = {}\n",
    "    all_unseen_test_gms['shared'] = [0]\n",
    "    all_unseen_test_gms['separate'] = [0]\n",
    "\n",
    "    optimizers = {}\n",
    "    optimizers['shared'] = optim.Adam(models['shared'].parameters(), lr=0.001)\n",
    "    optimizers['viewpoint'] = optim.Adam(models['viewpoint'].parameters(), lr=0.001)\n",
    "    optimizers['category'] = optim.Adam(models['category'].parameters(), lr=0.001)\n",
    "    for epoch in tqdm(range(5)):\n",
    "        train_gm_separate = 1\n",
    "        test_gm_separate = 1\n",
    "        unseen_test_gm_separate = 1\n",
    "\n",
    "        for TASK in ['viewpoint','category','shared']:\n",
    "            print('Epoch: %s, Task: %s'%(epoch,TASK))\n",
    "            print('---------')\n",
    "            models[TASK], train_loss, train_gm = train_epoch(dset_loaders, dset_sizes, models[TASK], TASK, optimizers[TASK])\n",
    "            best_models[TASK], test_loss, test_gm, best_test_loss, best_test_gm = test_epoch(dset_loaders, dset_sizes, models[TASK], best_models[TASK], best_test_loss, best_test_gm, TASK)\n",
    "            unseen_test_loss, unseen_test_gm = unseen_test_epoch(dset_loaders, dset_sizes, models[TASK], TASK)\n",
    "\n",
    "            if TASK != 'shared':\n",
    "                train_gm_separate = train_gm_separate * train_gm\n",
    "                test_gm_separate = test_gm_separate * test_gm\n",
    "                unseen_test_gm_separate = unseen_test_gm_separate * test_gm\n",
    "\n",
    "        all_train_gms['separate'].append(np.sqrt(train_gm_separate))\n",
    "        all_test_gms['separate'].append(np.sqrt(test_gm_separate))\n",
    "        all_unseen_test_gms['separate'].append(np.sqrt(unseen_test_gm_separate))\n",
    "        all_train_gms['shared'].append(train_gm)\n",
    "        all_test_gms['shared'].append(test_gm)\n",
    "        all_unseen_test_gms['shared'].append(np.sqrt(unseen_test_gm))\n",
    "\n",
    "    fig,ax = plt.subplots(1, 3, figsize=(18,6))\n",
    "    fig.suptitle(dataset_titles[DATASET_NAME], fontsize = 30)\n",
    "    l1 = ax[0].plot(all_train_gms['separate'], color = 'blue', marker = 'o', markersize=5)[0]\n",
    "    l2 = ax[0].plot(all_train_gms['shared'], color = 'red', marker = 'o', markersize=5)[0]\n",
    "    ax[0].set_title('Train Accuracy', fontsize=12)\n",
    "    line_labels = [\"Separate\", \"Shared\"]\n",
    "\n",
    "    ax[1].plot(all_test_gms['separate'], color = 'blue', marker = 'o', markersize=5)\n",
    "    ax[1].plot(all_test_gms['shared'], color = 'red', marker = 'o', markersize=5)\n",
    "    ax[1].set_title('Test Accuracy on Seen \\n Category-Viewpoint Combinations', fontsize=12)\n",
    "\n",
    "    ax[2].plot(all_unseen_test_gms['separate'], color = 'blue', marker = 'o', markersize=5)\n",
    "    ax[2].plot(all_unseen_test_gms['shared'], color = 'red', marker = 'o', markersize=5)\n",
    "    ax[2].set_title('Test Accuracy on Unseen \\n Category-Viewpoint Combinations', fontsize=12)\n",
    "    fig.legend([l1, l2],     # The line objects\n",
    "            labels=line_labels,   # The labels for each line\n",
    "            loc=\"center right\",   # Position of legend\n",
    "            borderaxespad=0.2,    # Small spacing around legend box\n",
    "            prop={\"size\":20})\n",
    "    plt.subplots_adjust(right=0.85, top =0.80)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462bfeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "increasing_seen_combinations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "generalization_to_ood",
   "language": "python",
   "name": "generalization_to_ood"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
